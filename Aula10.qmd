---
title: "Álgebra Linear Computacional"
subtitle: "Aula 10: Decomposição por valores singulares"
author: Heitor S. Ramos <br> <a href="mailto:ramosh@dcc.ufmg.br">ramosh@dcc.ufmg.br</a>
# date: 03/08/2022 
# abstract-title: 
format:
  revealjs: 
    code-fold: true 
    execute: 
      enabled: true
    echo: true
    jupyter: python3
    slide-number: true
    theme: default
    controls: true
    progress: true
    # smaller: true
    # scrollable: true
    chalkboard:
      buttons: true
      chalk-width: 1
      theme: whiteboard
    preview-links: auto
    logo:  common/UFMG_HEAD.png
    style: common/style.css
    footer: <https://heitorramos.github.io/alc.html>
search: true
resources:
  - aula10.pdf
---
## Créditos
:::{.callout-important}
Os slides desse curso são fortemente baseados no curso do Fabrício Murai e do Erickson Nascimento
:::

## Objetivos de aprendizagem

1. Conhecer relação entre posto, valores singulares e vetores singulares esquerdos e direitos de uma matriz
2. Saber transformar o SVD completo em SVD reduzido e vice-versa
3. Entender o SVD como uma decomposição em soma de matrizes de posto 1
4. Saber como calcular o SVD a partir da decomposição espectral de $Aˆ\top A$ ou $AAˆ\top$
Entender a multiplicação $Ax$ da perspectiva geométrica do SVD

## Referências adicionais

- [Wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition)
- YouTube:
    - [Steve Brunton’s series on SVD (U. Washington)](https://www.youtube.com/watch?v=gXbThCXjZFM&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv)
    - [Exemplo numérico de SVD](https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm)


## E quando a matriz não é quadrada?

$$A = \begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}$$

$$y = \begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}\begin{bmatrix}10\\100\end{bmatrix} = 10\begin{bmatrix}1\\3\\5\end{bmatrix}+100\begin{bmatrix}2\\4\\6\end{bmatrix} = y\in \mathbb R^3$$

## Exemplo de uma matriz tall-thin
![Tall-thin matrix](figs/Aula10/tallthin.png){width="50%"}

## E como ficam os autovetores?

Não é possível usarmos $Ax=\lambda x$

O que podemos fazer? Será que existem vetores escalares tais que

$$Av_i = \sigma_i u_i $$


## Quantos vetoes independentes de $u_i$?

Como $u_i$ está em $C(A)$, existem no máximo $r$ vetores $u_i$'s independentes, onde $r=\text{posto}(A)$

Note que sempre existe um $v_i$ para cada $u_i$

$$Av_1 = \sigma_1 u_1$$
$$Av_2 = \sigma_2 u_2$$

$$ \vdots$$

$$Av_\gamma = \sigma_\gamma u_\gamma $$


## Em forma matricial{.scrollable}

$$A\begin{bmatrix}\vert & \vert & \ldots & \vert \\ v_1 & v_2 & \ldots & v_r\\\vert & \vert & \ldots & \vert  \end{bmatrix} = \begin{bmatrix}\vert & \vert & \ldots & \vert \\ u_1 & u_2 & \ldots & u_r\\\vert & \vert & \ldots & \vert \end{bmatrix}\begin{bmatrix}\sigma_1 & & \\ &\ddots&\\&&\sigma_r\end{bmatrix} $$

:::{.callout-note}
Essas dimensões correspondem ao SVD reduzido
:::

## Exemplo $AV = U\Sigma$

$$\begin{bmatrix}3&0\\4&5\end{bmatrix}\frac{1}{\sqrt{2}}\begin{bmatrix}1&-1\\1&1\end{bmatrix} = \frac{1}{\sqrt{10}}\begin{bmatrix}1&-3\\3&1\end{bmatrix}\begin{bmatrix}3\sqrt{5}&\\&3\sqrt{5}\end{bmatrix} $$

## Tornando $V$ quadrada {.scrollable}
istem $r$ vetores $\sigma_iu_i$ independentes que podem ser escritos como $Av_i$

Sabemos ainda que o espaço nulo $N(A)$ tem dimensão $n-r$

Logo, existem $n-r$ vetores $v_j$ independentes tais que $Av_j=0$

Esses vetores são orgotonais a $v_1,\ldots,v_r$

$$
A\underbrace{\begin{bmatrix}
\vert & \ldots & \vert & & \vert& \ldots &\vert\\ 
v_1 &\ldots & v_r && v_{r+1} & \ldots & v_n\\
\vert & \ldots & \vert & & \vert& \ldots &\vert
\end{bmatrix}}_{m\times n}  = 
\underbrace{\begin{bmatrix}
\vert & \ldots & \vert\\
u_1 & \ldots & u_r\\
\vert & \ldots & \vert\\
\end{bmatrix}}_{m\times r}
\underbrace{\begin{bmatrix}
\sigma_1 &\ldots & 0 && 0 &\ldots & 0\\
\vdots &\ddots & \vdots && \vdots &\ddots & \vdots\\
0 &\ldots & \sigma_r && 0 &\ldots & 0\\
\end{bmatrix}}_{r\times n}
$$

## Em forma matricial

Se todos os vetores $v$'s forem ortogonais (e eles são), podemos reescrever como

$$ AV = U \Sigma$$

$$ AVV^{-1} = U \Sigma V^{-1}$$

$$ A = U \Sigma V^\top$$

## Do SVD completo ao reduzido{.scrollable}

:::{.callout-note}
## SVD Completo
<!-- >> SVD Completo -->

 $$\underbrace{A}_{m\times n} = \underbrace{U}_{m\times m} \underbrace{\Sigma}_{m\times n}\underbrace{V^\top}_{n\times n}$$
:::

Como apenas os $r$ primeiros termos da diagonal de $\Sigma$ são não-nulos, podemos tomar essa submatriz descartando as colunas $r+1,\ldots,m$ de $U$ e as linhas $r+1,\ldots,n$ de $V^\top$. O resultado, denotado pelas mesmas letras é

:::{.callout-note}
## SVD Reduzido
 $$\underbrace{A}_{m\times n} = \underbrace{U}_{m\times r} \underbrace{\Sigma}_{r\times r}\underbrace{V^\top}_{r\times n}$$
:::

## Decomposição por valor singular

A decomposição $A = U\Sigma V^\top$ é chamada de *Singular Value Decomposition (SVD)*

- Os escalares na matriz $\Sigma$ são os valores singulares
- Os vetores coluna de $U$ são os vetores singulares à esquerda
- Os vetores coluna de $V$ (ou seja, linhas de $V^\top$) são os vetores singulares à direita

## SVD em "pedaços"

Podemos escrever a decomposição como soma de matrizes de posto 1

$$ A = U \Sigma V^\top = \sigma_1u_1v_1^\top + \ldots + \sigma_r u_r v_r^\top$$

## Exemplo

$$ \frac{3\sqrt{5}}{\sqrt{10}\sqrt{2}}\begin{bmatrix}1\\3\end{bmatrix}\begin{bmatrix}1 & 1\end{bmatrix} +\frac{\sqrt{5}}{\sqrt{10}\sqrt{2}}\begin{bmatrix}-3\\1\end{bmatrix}\begin{bmatrix}-1 & 1\end{bmatrix}$$

$$ = \frac{3}{2}\begin{bmatrix}1&1\\3&3\end{bmatrix} + \frac{1}{2}\begin{bmatrix}3&-3\\-1&1\end{bmatrix} = \begin{bmatrix}3&0\\4&5\end{bmatrix}$$

## Autovalores e valores singulares

Existe alguma relação entre autovalores e valores singulares?

$$A = U\Sigma V^\top$$
$$S = X\Lambda X^{-1} $$

## Autovalores e valores singulares

Existe alguma relação entre autovalores e valores singulares?

$$ A^\top A = ?$$

## Autovalores e valores singulares

Existe alguma relação entre autovalores e valores singulares?

$$ AA^\top  = ?$$

## Autovalores e valores singulares

- $V$ possui os autovetores de $A^\top A$
- $U$ possui os autovetores de $AA^\top$
- O quadrado dos valores da diagonal de $\Sigma$ são os autovalores de $A^\top A$ e $AA^\top$

## Decompondo A

1. Calcule os autovetores $v$'s de $A^\top A$
2. Faça $\sigma_k = \sqrt{\lambda_k}$
3. Então calcule:   
$$ u_k = \frac{Av_k}{\sigma_k} $$

## Geometria de SVD

Vamos começar por uma matriz de rotação $R$:

$$R(\theta) = 
\begin{bmatrix}
\cos(\theta) & -\sin(\theta)\\
\sin(\theta) & \cos(\theta)
\end{bmatrix}
$$

$$R(\pi/4) = 
\begin{bmatrix}
\cos(\pi/4) & -\sin(\pi/4)\\
\sin(\pi/4) & \cos(\pi/4)
\end{bmatrix}
$$

$$R(\pi/4) = 
\begin{bmatrix}
1/\sqrt{2} & -1/\sqrt{2}\\
1/\sqrt{2}& 1/\sqrt{2}
\end{bmatrix}
$$

## Geometria de SVD

Vamos começar por uma matriz de rotação $R$:
$$R(\pi/4)x = 
\begin{bmatrix}
1/\sqrt{2} & -1/\sqrt{2}\\
1/\sqrt{2}& 1/\sqrt{2}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
= ?
$$

## Geometria de SVD

O que significa multiplicar um vetor $v$ por $A$

$$A v = (U\Sigma V^\top)v =  $$

## Geometria de SVD
![](figs/Aula10/geometria1.png){fig-align="center"}

## Geometria de SVD

$$ \Sigma(\sigma_1, \sigma_2) = 
\begin{bmatrix}
\sigma_1 & 0\\
0 & \sigma_2
\end{bmatrix}
$$

$$ \Sigma(2.0, 0.5) = 
\begin{bmatrix}
2.0 & 0\\
0 & 0.5
\end{bmatrix}
$$

## Geometria de SVD
![](figs/Aula10/geometria2.png){fig-align="center"}

## Geometria de SVD
![](figs/Aula10/geometria3.png){fig-align="center"}