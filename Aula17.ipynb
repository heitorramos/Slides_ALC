{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Álgebra Linear Computacional\"\n",
        "subtitle: \"Aula 17: Coeficiente de Determinação / Resolução do problema de Mínimos Quadrados\"\n",
        "author: Heitor S. Ramos <br> <a href=\"mailto:ramosh@dcc.ufmg.br\">ramosh@dcc.ufmg.br</a>\n",
        "# date: 03/08/2022 \n",
        "# abstract-title: \n",
        "format:\n",
        "  revealjs: \n",
        "    code-fold: true \n",
        "    execute: \n",
        "      enabled: true\n",
        "    echo: true\n",
        "    jupyter: python3\n",
        "    slide-number: true\n",
        "    theme: default\n",
        "    controls: true\n",
        "    progress: true\n",
        "    # smaller: true\n",
        "    # scrollable: true\n",
        "    chalkboard:\n",
        "      buttons: true\n",
        "      chalk-width: 1\n",
        "      theme: whiteboard\n",
        "    preview-links: auto\n",
        "    logo:  common/UFMG_HEAD.png\n",
        "    style: common/style.css\n",
        "    footer: <https://heitorramos.github.io/alc.html>\n",
        "search: true\n",
        "resources:\n",
        "  - aula10.pdf\n",
        "---"
      ],
      "id": "3804e734"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Créditos\n",
        ":::{.callout-important}\n",
        "Os slides desse curso são fortemente baseados no curso do Fabrício Murai e do Erickson Nascimento\n",
        ":::\n",
        "\n",
        "## Considerações sobre o coeficiente de determinação\n",
        "\n",
        "- Como avaliar se o modelo é bom?\n",
        "- Em geral, quando o vetor de resíduos for \"pequeno\", temos um bom ajuste\n",
        "\n",
        "$$\n",
        "\\Vert r \\Vert^2 = \\Vert Y - Y^2\\Vert = \\sum_{i=1}^n \\left(y_i - \\hat{y}\\right )^2\n",
        "$$\n",
        "\n",
        "\n",
        "## Considerações sobre o coeficiente de determinação{.scrollable}\n",
        "- Mas o ajuste é bom em relação a que?\n",
        "- Vamos usar o modelo mais simples, a média."
      ],
      "id": "fbe72d44"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|fig-align: \"center\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "x = np.array([18,19,20,24,25,27,28,29]).reshape(-1, 1)\n",
        "y = np.array([76.1, 77, 78.1, 79.9, 81.1, 81.8, 82.8, 83.5]).reshape(-1, 1)\n",
        "model = LinearRegression()\n",
        "model.fit(x, y)\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x,model.predict(x), color=\"red\")\n",
        "for i in range(8):\n",
        "    plt.vlines(x[i], y[i], model.predict(x)[i])\n",
        "plt.hlines(80.3, np.min(x), np.max(x), color=\"green\")\n",
        "for i in range(8):\n",
        "    plt.vlines(x[i], y[i], 80.3, color='grey')"
      ],
      "id": "38695f7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Considerações sobre o coeficiente de determinação{.scrollable}\n",
        "\n",
        "- $\\text{SQRes} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ (soma dos quadrados residual)\n",
        "- $\\text{SQTot} = \\sum_{i=1}^n (y_i - \\bar y)^2$ (soma dos quadrados total)\n",
        "- $\\text{SQReg} = \\sum_{i=1}^n (\\hat y_i - \\bar y)^2$ (soma dos quadrados devido à regressão)\n",
        "\n",
        "Dessa maneira,\n",
        "\n",
        "$$\n",
        "r^2 = \\frac{\\text{SQTot} - \\text{SQRes}}{\\text{SQTot}} = 1 - \\frac{\\text{SQRes}}{\\text{SQTot}}\n",
        "$$\n",
        "\n",
        "sendo $r^2$ denominado coeficiente de determinação e satisfazendo $0\\le r^2 \\le 1$\n",
        "\n",
        "## Mínimos quadrados\n",
        "\n",
        "\n",
        "$$ x^* = \\text{arg}\\min_x \\Vert Ax -b \\Vert^2 $$ \n",
        "\n",
        "\n",
        "Tendo como equações normais\n",
        "\n",
        "\n",
        "$$A^\\top A x = A^\\top b$$ \n",
        "\n",
        "\n",
        "ou \n",
        "\n",
        "\\begin{align}\n",
        "(A^\\top A)^{-1}A^\\top A x &= (A^\\top A)^{-1}A^\\top b \\\\ \n",
        "x &= (A^\\top A)^{-1}A^\\top b \n",
        "\\end{align}\n",
        "\n",
        "## Moore-Penrose pseudoinversa {.scrollable}\n",
        "\n",
        "Considere \n",
        "\n",
        "$$A^\\top A x = A^\\top b$$ \n",
        "\n",
        "\n",
        "Toda matriz $A = U\\Sigma V^\\top$\n",
        "\n",
        "Assim\n",
        "\n",
        "\n",
        "$$ (U\\Sigma V^\\top)^\\top (U\\Sigma V^\\top) x = (U\\Sigma V^\\top)^\\top b$$\n",
        "\n",
        "$$V\\Sigma^\\top U^\\top U\\Sigma V^\\top x = \n",
        "V\\Sigma^\\top U^\\top b$$\n",
        "\n",
        "\n",
        "\n",
        "$$V\\Sigma^\\top \\Sigma V^\\top x = V\\Sigma^\\top U^\\top b$$\n",
        "\n",
        "$$V\\Sigma^\\top \\Sigma V^\\top x = V\\Sigma^\\top U^\\top b$$\n",
        "\n",
        "$$V\\Sigma^2  V^\\top x = V\\Sigma^\\top U^\\top b$$\n",
        "\n",
        "$$V^\\top V\\Sigma^2  V^\\top x = V^\\top V\\Sigma^\\top U^\\top b$$\n",
        "\n",
        "$$\\Sigma^2  V^\\top x =\\Sigma^\\top U^\\top b$$\n",
        "\n",
        "$$V^\\top x =\\frac{\\Sigma}{\\Sigma^2} U^\\top b$$\n",
        "\n",
        "$$x = V\\Sigma^{-1} U^\\top b$$\n",
        "\n",
        "\n",
        "## Exercício{.scrollable}\n",
        "\n",
        "Calcule a pseudoinversa da matrix $A = \\begin{bmatrix}1 & 2 \\\\ 1 & 2\\end{bmatrix}$\n",
        "\n",
        "\n",
        "$$ A^\\top A = \\begin{bmatrix}1 & 1 \\\\ 2 & 2\\end{bmatrix}\\begin{bmatrix}1&2\\\\ 1&2\\end{bmatrix} = \\begin{bmatrix}2&4\\\\ 4&8\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "Polinômio característico:\n",
        "\n",
        "$$(2- \\lambda)(8 - \\lambda) -16 = 0$$\n",
        "\n",
        "$$16 - 2\\lambda - 8\\lambda +\\lambda^2 -16 = 0$$\n",
        "\n",
        "$$ \\lambda^2 - 10\\lambda = 0$$\n",
        "\n",
        "$$\\lambda = 10 \\text{ e } \\lambda = 0$$\n",
        "\n",
        "\n",
        "Calculando autovetor 1:\n",
        "\n",
        "\n",
        "$$AX_1 = \\lambda_1 X_1$$\n",
        "\n",
        "$$\\begin{bmatrix}2&4\\\\ 4 & 8\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} = \\begin{bmatrix}10x_1\\\\10x_2\\end{bmatrix}$$\n",
        "\n",
        "$$\\begin{bmatrix}2x_1+4x_2\\\\ 4x_1 + 8x_2\\end{bmatrix} = \\begin{bmatrix}10x_1\\\\10x_2\\end{bmatrix}$$\n",
        "\n",
        "$$\\begin{bmatrix}-8x_1+4x_2\\\\ 4x_1 - 2x_2\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$$\n",
        "\n",
        "$$-8x_1 = -4x_2 \\Rightarrow x_1=\\frac{1}{2}x_2$$\n",
        "\n",
        "\n",
        "Normalizando o vetor $X_1 = \\begin{bmatrix}-\\frac{1}{2}& -1\\end{bmatrix}^\\top$, temos $X_1 = \\begin{bmatrix}-0.4472136&-0.89442719 \\end{bmatrix}^\\top$\n",
        "\n",
        "Calculando autovetor 2\n",
        "\n",
        "$$AX_2 = \\lambda_1 X_2$$\n",
        "\n",
        "$$\\begin{bmatrix}2&4\\\\ 4 & 8\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$$\n",
        "\n",
        "$$\\begin{bmatrix}2x_1+4x_2\\\\ 4x_1 + 8x_2\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$$\n",
        "\n",
        "$$2x_1 = -4x_2 \\Rightarrow x_1=-2x_2$$\n",
        "\n",
        "\n",
        "Normalizando o vetor $X_2 = \\begin{bmatrix}-2&1\\end{bmatrix}^\\top$, temos $X_2 = \\begin{bmatrix}-0.89442719& 0.4472136 \\end{bmatrix}^\\top$\n",
        "\n",
        "Sendo assim, temos a decomposição SVD de $A$ como\n",
        "\n",
        "\n",
        "$$A = U\\Sigma V^\\top$$\n",
        "\n",
        "\n",
        "com \n",
        "\n",
        "$$V^\\top = \\begin{bmatrix}-0.89442719& -0.4472136\\\\ 0.4472136& - 0.89442719\\end{bmatrix}$$\n",
        "\n",
        "$$\\Sigma = \\begin{bmatrix}0 & 0 \\\\ 0 & \\sqrt{10} \\end{bmatrix}$$\n",
        "\n",
        "$$U = AV\\Sigma^{-1} = \\begin{bmatrix}1 & 2 \\\\ 1 & 2\\end{bmatrix}\\begin{bmatrix}-0.89442719 & -0.4472136 \\\\ 0.4472136& - 0.89442719 \\end{bmatrix}\\begin{bmatrix}0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{10}} \\end{bmatrix}$$\n",
        "\n",
        "\n",
        "continua.... (ir para o Jupyter)\n",
        "\n",
        "## Decomposição QR\n",
        "\n",
        "- E se as colunas de $A$ forem linearmente independentes?\n",
        "\n",
        "- Não precisamos de pseudoinversa!\n",
        "\n",
        "- Podemos procurar por matrizes fáceis de serem invertidas\n",
        "    - Ex: matrizes triangulares e ortogonais\n",
        "\n",
        "## Decomposição QR{.scrollable}\n",
        "\n",
        "- Matrizes triangulares com elementos na diagonal não nulos  são fáceis de inverter\n",
        "\n",
        "- Matrizes ortogonais também são fáceis de inverter $Q^\\top = Q^{-1}$\n",
        "\n",
        "\n",
        "$$Q_i^\\top Q_j = \\begin{cases}1, \\text{ se }i=j\\\\ 0, \\text{ se }i \\neq j\\end{cases}$$\n",
        "\n",
        "\n",
        "A ideia é conseguir decompor a matriz $A$ em  duas matrizes fáceis de inverter, e.g., uma ortogonal e outra triangular, ou seja, $A = QR$\n",
        "\n",
        "## Equações normais{.scrollable}\n",
        "\n",
        "$A^\\top A x = A^\\top b$, com  $A = QR$\n",
        "\n",
        "\n",
        "$$ (QR)^\\top (QR) x = (QR)^\\top b$$\n",
        "\n",
        "$$ R^\\top Q^\\top Q R x = R^\\top Q^\\top b$$\n",
        "\n",
        "$$ R^\\top R x = R^\\top Q^\\top b$$\n",
        "\n",
        "$$(R^\\top)^{-1}(R^\\top) R x = (R^\\top)^{-1}R^\\top Q^\\top b $$\n",
        "\n",
        "$$ R x = Q^\\top b$$\n",
        "\n",
        "$$ (R^{-1})R x = R^{-1} Q^\\top b$$\n",
        "\n",
        "$$x = R^{-1}Q^\\top b $$\n",
        "\n",
        "\n",
        "\n",
        "## Decomposição QR\n",
        "\n",
        "- Se tivermos a decomposição $A = QR$, com $Q^\\top Q = I$ e $R$ inversível, já sabemos como usá-la para obter os coeficientes $\\hat{\\beta}_i$ em uma regressão.\n",
        "\n",
        "- Mas como obter $A = QR$?\n",
        "- Existe mais de uma maneira de obter\n",
        "- Vamos conhecer um algoritmo baseado na ortogonalização de Gram-Schmidt\n",
        "- O objetivo agora é transformar $A$ em $Q$, onde cada coluna $q_i$ será ortonormal, ou seja, faremos $q_i = \\frac{A_i}{\\Vert A_i\\Vert}$\n",
        "\n",
        "## Projeção ortogonal \n",
        "![](figs/Aula17/proj_ort.png){fig-align=\"center\"}\n",
        "\n",
        "## Projeção ortogonal \n",
        "![](figs/Aula17/proj_ort2.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## Gram-Schimidt\n",
        "![](figs/Aula17/gs1.png){fig-align=\"center\"}\n",
        "\n",
        "## Gram-Schimidt\n",
        "![](figs/Aula17/gs2.png){fig-align=\"center\"}\n",
        "\n",
        "## Gram-Schimidt\n",
        "![](figs/Aula17/gs3.png){fig-align=\"center\"}\n",
        "\n",
        "## Como encontrar a matriz R?\n",
        "\n",
        "\n",
        "$$ A = QR$$\n",
        "\n",
        "$$Q^\\top A = Q^\\top Q R$$\n",
        "\n",
        "$$R = Q^\\top A$$"
      ],
      "id": "7115259b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}